{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBHv1id7uYWf"
      },
      "source": [
        "#OCR a PDF in Portuguese and Split into Phrases with English Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Enter URL and click Runtime>Run All"
      ],
      "metadata": {
        "id": "Gv0FN3EwOZaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XndI2KuMO3TH"
      },
      "outputs": [],
      "source": [
        "url = 'http://objdigital.bn.br/objdigital2/acervo_digital/div_obrasgerais/drg177349/drg177349.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCR"
      ],
      "metadata": {
        "id": "UZ7Ww467O9Rz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOIo6UG3pv66"
      },
      "outputs": [],
      "source": [
        "!pip install pdf2image\n",
        "!pip install PyPDF2\n",
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install sentencepiece\n",
        "!pip install --upgrade transformers\n",
        "!apt-get install poppler-utils "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBh51xGRpr-V"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import io\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "import concurrent.futures\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CViFN3qBNr-D"
      },
      "outputs": [],
      "source": [
        "response = requests.get(url)\n",
        "\n",
        "# Extract the filename from the URL\n",
        "parsed = urlparse(url)\n",
        "filename = os.path.basename(parsed.path)\n",
        "\n",
        "with open(filename, 'wb') as f:\n",
        "    f.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70iuoh_SrOtL"
      },
      "outputs": [],
      "source": [
        "# Extract filename without extension\n",
        "doc_name = os.path.splitext(os.path.basename(parsed.path))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX2q8xOUp_99"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    # Convert to YCrCb color space\n",
        "    img_ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    # Apply a sharp S-curve to Y channel\n",
        "    y, cr, cb = cv2.split(img_ycrcb)\n",
        "    y = np.clip(y * 1.9 - 100, 0, 255).astype(np.uint8)\n",
        "    img_ycrcb = cv2.merge((y, cr, cb))\n",
        "\n",
        "    # Convert back to BGR color space\n",
        "    img_bgr = cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2BGR)\n",
        "\n",
        "    # Reduce reds and yellows\n",
        "    b, g, r = cv2.split(img_bgr)\n",
        "    r = np.clip(r * 0.8, 0, 255).astype(np.uint8)\n",
        "    g = np.clip(g * 0.9, 0, 255).astype(np.uint8)\n",
        "    img_reduced = cv2.merge((b, g, r))\n",
        "\n",
        "    # Convert to grayscale\n",
        "    img_gray = cv2.cvtColor(img_reduced, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply median blur\n",
        "    img_blur = cv2.medianBlur(img_gray, 1)\n",
        "\n",
        "    # Apply Otsu's thresholding\n",
        "    _, img_thresh = cv2.threshold(img_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    return img_thresh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gP_11LdOf10"
      },
      "outputs": [],
      "source": [
        "# Download Portugese training data for tesseract\n",
        "url = 'https://github.com/tesseract-ocr/tessdata/raw/main/por.traineddata'\n",
        "response = requests.get(url)\n",
        "\n",
        "with open('por.traineddata', 'wb') as f:\n",
        "    f.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF9yXZukKcHJ"
      },
      "outputs": [],
      "source": [
        "# Move to the required folder\n",
        "!mv por.traineddata /usr/share/tesseract-ocr/4.00/tessdata/por.traineddata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dbDyo-ZqWu5"
      },
      "outputs": [],
      "source": [
        "pdf_path = f'/content/{doc_name}.pdf'\n",
        "output_filename = f\"/content/{doc_name}.txt\"\n",
        "\n",
        "pdf_reader = PdfReader(open(pdf_path, \"rb\"))\n",
        "total_pages = len(pdf_reader.pages)\n",
        "\n",
        "# Batching keeps the memory from overloading - especially on larger pdf files\n",
        "batch_size = 10\n",
        "num_batches = (total_pages + batch_size - 1) // batch_size\n",
        "\n",
        "# Create folder for images\n",
        "sub_dir = str(f\"/content/{doc_name}/\")\n",
        "if not os.path.exists(sub_dir):\n",
        "    os.makedirs(sub_dir)\n",
        "\n",
        "# Loop through, preprocessing images and OCRing\n",
        "for batch in range(num_batches):\n",
        "    start_page = batch * batch_size\n",
        "    end_page = min((batch + 1) * batch_size, total_pages)\n",
        "    pages = convert_from_path(pdf_path, first_page=start_page, last_page=end_page - 1)\n",
        "\n",
        "    for i, page in enumerate(pages):\n",
        "        pg_cntr = start_page + i + 1\n",
        "        filename = f\"pg_{str(pg_cntr)}_{doc_name}.jpg\"\n",
        "        page.save(sub_dir + filename)\n",
        "\n",
        "        # Load the saved image and preprocess it\n",
        "        img = cv2.imread(sub_dir + filename)\n",
        "        preprocessed_img = preprocess_image(img)\n",
        "        img_pil = Image.fromarray(preprocessed_img)\n",
        "\n",
        "        # Save preprocessed image\n",
        "        preprocessed_filename = \"preprocessed_\" + filename\n",
        "        cv2.imwrite(sub_dir + preprocessed_filename, preprocessed_img)\n",
        "\n",
        "        with io.open(output_filename, 'a+', encoding='utf8') as f:\n",
        "            f.write(pytesseract.image_to_string(f\"/content/{doc_name}/{preprocessed_filename}\", lang='por') + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separate OCRed Text Into Phrases"
      ],
      "metadata": {
        "id": "cfKC405ePKiB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdxDGQnc0ZcG"
      },
      "outputs": [],
      "source": [
        "path = \"/content/\"\n",
        "\n",
        "# get a list of all files in the directory\n",
        "files = os.listdir(path)\n",
        "\n",
        "# filter the list to include only files with a .txt extension\n",
        "txt_files = [file for file in files if file.endswith(\".txt\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKoIZbIl1JbX"
      },
      "outputs": [],
      "source": [
        "chunk_size = 10000  \n",
        "delimiters = [';', '—', '.', '?', '!', ':']\n",
        "\n",
        "# Define patterns for initials and titles\n",
        "INITIAL_PATTERN = re.compile(r'^[A-Z]\\.$')\n",
        "TITLE_PATTERN = re.compile(r'^[A-Z][a-z]+\\s[A-Z][a-z]+$')\n",
        "\n",
        "phrases = []\n",
        "for i in range(len(txt_files)):\n",
        "    example = txt_files[i]\n",
        "    with open(path + example, 'r', encoding='utf-8') as f:\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "\n",
        "            # Remove hyphen and newline for pattern1\n",
        "            chunk = re.sub(r'([a-zA-Z])-\\n([a-zA-Z])', r'\\1\\2', chunk)\n",
        "            chunk = re.sub(r'\\n', r' ', chunk)\n",
        "            # Split the chunk into phrases\n",
        "            phrase_start = 0\n",
        "            for i, c in enumerate(chunk):\n",
        "                if c in delimiters:\n",
        "                    # Check if the delimiter is part of a title or initial or hyphenated word\n",
        "                    if i > 1 and (INITIAL_PATTERN.match(chunk[i-2:i+1]) or TITLE_PATTERN.match(chunk[phrase_start:i+1])):\n",
        "                        continue\n",
        "\n",
        "                    # Add the phrase to the list\n",
        "                    phrases.append(chunk[phrase_start:i+1])\n",
        "                    phrase_start = i+1\n",
        "\n",
        "            # Add the last phrase to the list\n",
        "            if phrase_start < len(chunk):\n",
        "                phrases.append(chunk[phrase_start:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9pZvm0k1J5A"
      },
      "outputs": [],
      "source": [
        "len(phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean up characters that shouldn't have been recognized and drop empty phrases"
      ],
      "metadata": {
        "id": "kZqyGnLbPVgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LwJ9EJP1Odo"
      },
      "outputs": [],
      "source": [
        "# Define a regex pattern to match all non-Portuguese letters and non-valid punctuation\n",
        "pattern = re.compile(r'[^a-zA-Zà-úÁ-Ú0-9,.?!:;()\"\\' ]')\n",
        "\n",
        "# Clean each phrase in the list of phrases\n",
        "cleaned_phrases = []\n",
        "for phrase in phrases:\n",
        "    cleaned_phrase = re.sub(pattern, '', phrase)\n",
        "    \n",
        "    # Remove any phrases that do not contain any Portuguese letters\n",
        "    if not re.search(r'[à-úÁ-Úa-zA-Z]', cleaned_phrase):\n",
        "        continue\n",
        "    \n",
        "    # Remove any extra spaces at the start or end of the phrase\n",
        "    cleaned_phrase = cleaned_phrase.strip()\n",
        "    \n",
        "    # Remove multiple spaces in the middle of the phrase\n",
        "    cleaned_phrase = re.sub(r'\\s+', ' ', cleaned_phrase)\n",
        "    \n",
        "    cleaned_phrases.append(cleaned_phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKQAV1Qy1REU"
      },
      "outputs": [],
      "source": [
        "len(cleaned_phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translate each of the phrases and export to json every 1000 examples"
      ],
      "metadata": {
        "id": "x51cSF-VPiqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7vHE1qb1av5"
      },
      "outputs": [],
      "source": [
        "src_language = 'pt'\n",
        "tgt_language = 'en'\n",
        "\n",
        "# Load the tokenizer and model for the language pair\n",
        "model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "translated_phrases = []\n",
        "\n",
        "def translate(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return [text, translation]\n",
        "\n",
        "# Define the number of threads to use\n",
        "num_threads = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuwifCXh1gws",
        "outputId": "821fb0ad-58d3-4aaf-ccd6-2774136f8a71"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "translated_phrases = []\n",
        "num_threads = 8\n",
        "\n",
        "# Process the phrases using a thread pool\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # Submit the translation tasks and store the Future objects in a list\n",
        "    futures = [executor.submit(translate, text) for text in cleaned_phrases]\n",
        "\n",
        "    # Iterate over the completed Future objects and collect the results\n",
        "    count = 0\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        result = future.result()\n",
        "        translated_phrases.append(result)\n",
        "        count += 1\n",
        "        \n",
        "        # Write translations to a file every 1000 phrases\n",
        "        if count % 1000 == 0:\n",
        "            with open(f'ptbr_phrases_{doc_name}_{(count // 1000)}.json', 'w') as f:\n",
        "                json.dump(translated_phrases[-1000:], f)\n",
        "    \n",
        "    # Write any remaining translations to a file\n",
        "    if translated_phrases:\n",
        "        with open(f'ptbr_phrases_{doc_name}_{count // 1000 + 1}.json', 'w') as f:\n",
        "            json.dump(translated_phrases, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ4ZTx6KOjqL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}